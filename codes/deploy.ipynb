{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPLOY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display as display\n",
    "import biosppy\n",
    "import scipy.signal as sig\n",
    "from scipy.stats import zscore\n",
    "from scipy.io import wavfile\n",
    "from scipy.fft import fft,fftfreq\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "mlp.rc(\"xtick\",labelsize=10)\n",
    "mlp.rc(\"ytick\",labelsize=10)\n",
    "mlp.rc(\"axes\",labelsize=11)\n",
    "plt.rcParams[\"figure.figsize\"] = [11,5]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "CURR_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR = os.getcwd()\n",
    "\n",
    "MAIN_DIR = \".\"\n",
    "if os.path.basename(os.getcwd())!=\"Silent-Interface-for-IOT-Devices\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "PICKLE_DIR = os.path.join(MAIN_DIR,\"pickles\")\n",
    "MODEL_DIR = os.path.join(MAIN_DIR,\"models\")\n",
    "FONT_DIR = os.path.join(MAIN_DIR,\"fonts\")\n",
    "TEST_DIR = os.path.join(MAIN_DIR,\"test\")\n",
    "\n",
    "SCALE_FACTOR = 0.022351744455307063\n",
    "# dataset definitions\n",
    "SENTENCES =[\"अबको समय सुनाउ\",\"एउटा सङ्गित बजाउ\",\"आजको मौसम बताउ\",\"बत्तिको अवस्था बदल\",\"पङ्खाको स्तिथी बदल\"]\n",
    "LABELS = np.array(SENTENCES)[[3, 2, 1, 4, 0, 3, 2, 1, 4, 0, 3, 2, 1, 4, 0]]\n",
    "\n",
    "\n",
    "ORDERED_SENTENCES =[\"अबको समय सुनाउ\",\"आजको मौसम बताउ\",\"एउटा सङ्गित बजाउ\",\"पङ्खाको स्तिथी बदल\",\"बत्तिको अवस्था बदल\"]\n",
    "# ordered\n",
    "# TEST_LABELS = np.array([4.0, 1.0, 2.0, 3.0, 0.0, 4.0, 1.0, 2.0, 3.0, 0.0, 4.0, 1.0, 2.0, 3.0, 0.0]) #real dataset\n",
    "\n",
    "\n",
    "# unordered\n",
    "# TEST_LABELS = np.array([0,1,2,0,3,1,0,3,0,0,1,1,3,3,4,4,2,3,1,2,2,2,4,4,4]) * 1.0 #sample dataset\n",
    "\n",
    "# ordered\n",
    "# TEST_LABELS = np.array([0.0, 2.0, 1.0, 0.0, 4.0, 2.0, 0.0, 4.0, 0.0, 0.0, 2.0, 2.0, 4.0, 4.0, 3.0, 3.0, 1.0, 4.0, 2.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0]) #sample dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(INPUT_SHAPE,DROPOUT=0.3,learning_rate=0.0003,activation=\"relu\",neurons=64,K_regulizer=0.001):\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # 1st conv layer\n",
    "    model.add(keras.layers.Conv1D(64,(3),activation=\"relu\",input_shape=INPUT_SHAPE,\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling1D((3), strides=(2), padding='same'))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Conv1D(32, (3), activation='relu',\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling1D((3), strides=(2), padding='same'))\n",
    "\n",
    "    # 3rd conv layer\n",
    "    model.add(tf.keras.layers.Conv1D(32, (2), activation='relu',\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling1D((2), strides=(2), padding='same'))\n",
    "\n",
    "\n",
    "    model.add(LSTM(units = 50, return_sequences = True))\n",
    "    tf.keras.layers.Dropout(0.3)\n",
    "    model.add(LSTM(units = 50, return_sequences = True))\n",
    "    tf.keras.layers.Dropout(0.3)\n",
    "    model.add(LSTM(units = 50, return_sequences = True))\n",
    "    tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "    # softmax output layer\n",
    "    model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "        \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1646, 64)          1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1646, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 823, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 821, 32)           6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 821, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 411, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 410, 32)           2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 410, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 205, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 205, 50)           16600     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 205, 50)           20200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 205, 50)           20200     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10250)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                656064    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 723,757\n",
      "Trainable params: 723,501\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = RNN(INPUT_SHAPE=(1648,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_weights(os.path.join(MODEL_DIR,'1DCNN-3alyer-LSTM-3alyer-30EPOCH.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARSE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(files,NORMALIZE=True,DEPLOY=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    parser function to extract utterances from .txt file and store them in a dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # PERCENTILES FOR LENGTH NORMALIZATION\n",
    "    if(NORMALIZE==True):\n",
    "        percentile_95 = 1602\n",
    "        percentile_97 = 1614\n",
    "        percentile_99 = 1648\n",
    "        percentile_100 = 1875\n",
    "    else:\n",
    "        percentile_95 = percentile_97 = percentile_99 = percentile_100 = -1\n",
    "        \n",
    "    dataset = {\"data\":[], \"speaker\":[],\"session\":[],\"labels\":[]}\n",
    "    \n",
    "    def get_data(file):\n",
    "        \n",
    "        signal = read_data(file)\n",
    "        \n",
    "        if(DEPLOY==True):\n",
    "            dataset[\"data\"].extend(signal)\n",
    "            return\n",
    "        \n",
    "#         if(len(signal)!=25):\n",
    "#             return\n",
    "        \n",
    "        session = file.split(\"/\")[-2]\n",
    "        speaker = file.split(\"/\")[-3]\n",
    "        \n",
    "        dataset[\"data\"].extend(signal)\n",
    "        dataset[\"speaker\"].extend([speaker]*len(signal))\n",
    "        dataset[\"session\"].extend([session]*len(signal))\n",
    "        dataset[\"labels\"].extend(LABELS)\n",
    "        \n",
    "    \n",
    "    def read_data(file):\n",
    "        f = open(file, 'r')\n",
    "        contents = map(lambda x : x.strip(), f.readlines())\n",
    "        #the file starts with '%' and some instruction before data and removing these data \n",
    "        frames_original = list(filter(lambda x : x and x[0] != '%', contents))[1:]\n",
    "        #the data row contains channels info digital trigger and accelerometer info separated by comma\n",
    "        frames_original = list(map(lambda s : list(map( lambda ss: ss.strip(), s.split(','))), frames_original))\n",
    "        # (8 channels) + digital triggers\n",
    "        # the digital trigger is in a[16], used to indicate the utterance\n",
    "        frames = list(map(lambda a: list(map(float, a[1:9])) + [float(a[16])] , frames_original))\n",
    "        frames = np.array(frames)\n",
    "        indices = []\n",
    "        signal = []\n",
    "        for index,f in enumerate(frames[:,-1]):\n",
    "            if(bool(f) ^ bool(frames[(index+1) if ((index+1)<len(frames)) else index,-1]) ):\n",
    "                indices.append(index)\n",
    "                if len(indices)>1 and len(indices)%2==0:\n",
    "                    frame_len = indices[len(indices)-1] - indices[len(indices)-2]\n",
    "                    if(frame_len<percentile_99):\n",
    "                        pad = int(np.ceil((percentile_99 - frame_len)/2))\n",
    "                    else:\n",
    "                        pad = 0\n",
    "                    left_pad = indices[len(indices)-2] - pad\n",
    "                    right_pad = indices[len(indices)-1] + pad\n",
    "                    a_frame = (frames[left_pad:right_pad,:-1])[:percentile_99]\n",
    "                    signal.append(a_frame)\n",
    "    \n",
    "        # convert to microVolts and return\n",
    "        return np.array(signal)*SCALE_FACTOR\n",
    "        \n",
    "#     for file,i in zip(files,tqdm.tqdm(range(1,len(files)+1),desc=\"PARSING DATA\")):\n",
    "#         get_data(file)\n",
    "\n",
    "\n",
    "    for file in files:\n",
    "        get_data(file)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS SIGNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_pipeline(data,RICKER=False):\n",
    "\n",
    "    \"\"\"\n",
    "    CORRECT DC DRIFT --> CORRECT DC BIAS --> SMOOTHING SIGNAL --> NORMALIZE DATA --> FILTER DATA \t\n",
    "    \"\"\"\n",
    "    filter_data = []\n",
    "\n",
    "    def digital_filter(data,HPF=0.5,LPF=10,H_ORDER=4,L_ORDER=4,SR=250):\n",
    "        \"\"\"\n",
    "        HPF --> NOTCH --> LPF --> RICKER CONVOLUTION\n",
    "        \"\"\"\n",
    "\n",
    "        # highpass filter\n",
    "        f_signal = biosppy.signals.tools.filter_signal(data,ftype=\"butter\",band=\"highpass\",order=H_ORDER,sampling_rate=SR,frequency=HPF)\n",
    "        # notch filter\n",
    "        b,a = sig.iirnotch(50,30,SR)\n",
    "        f_signal = sig.lfilter(b,a,f_signal[0])\n",
    "\n",
    "        # lowpass filter\n",
    "        f_signal = biosppy.signals.tools.filter_signal(f_signal,ftype=\"butter\",band=\"lowpass\",order=L_ORDER,sampling_rate=SR,frequency=LPF)\n",
    "\n",
    "        if(RICKER==True):\n",
    "            # RICKER CONVOLUTION TO REMOVE HEARTBEAT ARTIFACTS\n",
    "            ricker_width = 35 * SR // 250\n",
    "            ricker_sigma = 4.0 * SR / 250\n",
    "            ricker = sig.ricker(ricker_width,ricker_sigma)\n",
    "            # normalize ricker\n",
    "            ricker = np.array(ricker, np.float32) / np.sum(np.abs(ricker))\n",
    "            convolution = sig.convolve(f_signal[0],ricker,mode=\"same\")\n",
    "            return (f_signal[0]-2*convolution)\n",
    "\n",
    "        return f_signal[0]\n",
    "\n",
    "    def process_signal(data):\n",
    "        f_data = []\n",
    "        for i in range(8):\n",
    "            # correction of DC drift\n",
    "            c_data = data[:,i]- data[0,i]\n",
    "\n",
    "            # correct DC bias\n",
    "            c_data = c_data - np.mean(c_data)\n",
    "\n",
    "            # normalize and filter data\n",
    "            c_data = digital_filter(c_data)\n",
    "            f_data.append(c_data)\n",
    "\n",
    "        return np.array(f_data).T\n",
    "\n",
    "#     for d,i in zip(data,tqdm.tqdm(range(1,len(data)+1),desc=\"PROCESSING DATA: \")):\n",
    "#         temp_data = process_signal(d)\n",
    "#         filter_data.extend([temp_data])\n",
    "\n",
    "    for d in data:\n",
    "        temp_data = process_signal(d)\n",
    "        filter_data.extend([temp_data])\n",
    "        \n",
    "    return np.array(filter_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching filtered data from pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching filtered data from pickle file ...\")\n",
    "all_data_filtered = pickle.load(open(os.path.join(PICKLE_DIR,\"data_dict_filtered.pickle\"),\"rb\"))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(all_data_filtered,K_FOLD=False):\n",
    "    data = all_data_filtered[\"data\"]\n",
    "    labels = all_data_filtered[\"labels\"]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_labels = encoder.fit_transform(labels).astype('float64')\n",
    "\n",
    "    print(encoded_labels[:10])\n",
    "    print(all_data_filtered[\"labels\"][:10])\n",
    "\n",
    "    X = np.array(data)\n",
    "    Y = encoded_labels\n",
    "    \n",
    "    if(K_FOLD==True):\n",
    "        return X,Y\n",
    "    \n",
    "    splitter = StratifiedShuffleSplit(n_splits=1,test_size=0.1, random_state=42)\n",
    "\n",
    "    # train test split\n",
    "    train_id, test_id = next(splitter.split(X,Y))\n",
    "    X_train,y_train,X_test,y_test = X[train_id],Y[train_id],X[test_id],Y[test_id]\n",
    "\n",
    "    \n",
    "    # train val split\n",
    "    train_id, test_id = next(splitter.split(X_train,y_train))\n",
    "    X_train,y_train,X_val,y_val = X_train[train_id],y_train[train_id],X_train[test_id],y_train[test_id]\n",
    "    print(\"Shape of data instance: \", X_train[0].shape)\n",
    "    print(\"Shape of Training data: \",X_train.shape)\n",
    "    print(\"Shape of Testing data: \",X_test.shape)\n",
    "\n",
    "    return X_train,y_train,X_val,y_val,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 1. 2. 3. 0. 4. 1. 2. 3. 0.]\n",
      "['बत्तिको अवस्था बदल', 'आजको मौसम बताउ', 'एउटा सङ्गित बजाउ', 'पङ्खाको स्तिथी बदल', 'अबको समय सुनाउ', 'बत्तिको अवस्था बदल', 'आजको मौसम बताउ', 'एउटा सङ्गित बजाउ', 'पङ्खाको स्तिथी बदल', 'अबको समय सुनाउ']\n",
      "Shape of data instance:  (1648, 8)\n",
      "Shape of Training data:  (5284, 1648, 8)\n",
      "Shape of Testing data:  (653, 1648, 8)\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_val,y_val,X_test,y_test = prepare_dataset(all_data_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 94ms/step - loss: 0.4219 - accuracy: 0.8895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42186692357063293, 0.8894557952880859]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_pipeline(file):\n",
    "    data = parser(file,DEPLOY=True)[\"data\"]\n",
    "    data = signal_pipeline(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7478 - accuracy: 0.8000\n",
      "[0.7477979063987732, 0.800000011920929]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4213 - accuracy: 0.6667\n",
      "[1.4213029146194458, 0.6666666865348816]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3596 - accuracy: 0.5333\n",
      "[1.3595722913742065, 0.5333333611488342]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7502 - accuracy: 0.8000\n",
      "[0.7502457499504089, 0.800000011920929]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.9333\n",
      "[0.321938157081604, 0.9333333373069763]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.9333\n",
      "[0.38830137252807617, 0.9333333373069763]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9333\n",
      "[0.18722939491271973, 0.9333333373069763]\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8208 - accuracy: 0.8667\n",
      "[0.8208482265472412, 0.8666666746139526]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4749 - accuracy: 0.8667\n",
      "[0.47490864992141724, 0.8666666746139526]\n"
     ]
    }
   ],
   "source": [
    "# ordered\n",
    "TEST_LABELS = np.array([4.0, 1.0, 2.0, 3.0, 0.0, 4.0, 1.0, 2.0, 3.0, 0.0, 4.0, 1.0, 2.0, 3.0, 0.0]) #real dataset\n",
    "\n",
    "for i in range(1,11):\n",
    "    data = deploy_pipeline([\"test\"+str(i)+\".txt\"])\n",
    "    if(len(data)>15):\n",
    "        continue\n",
    "    print(model.evaluate(data,TEST_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4731 - accuracy: 0.2400\n",
      "[3.4731409549713135, 0.23999999463558197]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6576 - accuracy: 0.2400\n",
      "[3.6575863361358643, 0.23999999463558197]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8729 - accuracy: 0.4000\n",
      "[2.872880458831787, 0.4000000059604645]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6409 - accuracy: 0.3200\n",
      "[2.6409058570861816, 0.3199999928474426]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5988 - accuracy: 0.4400\n",
      "[2.5987653732299805, 0.4399999976158142]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2730 - accuracy: 0.0800\n",
      "[5.272961616516113, 0.07999999821186066]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3351 - accuracy: 0.1200\n",
      "[3.335120916366577, 0.11999999731779099]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1124 - accuracy: 0.2400\n",
      "[3.112440824508667, 0.23999999463558197]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0897 - accuracy: 0.3200\n",
      "[3.0896544456481934, 0.3199999928474426]\n"
     ]
    }
   ],
   "source": [
    "# unordered\n",
    "# TEST_LABELS = np.array([0,1,2,0,3,1,0,3,0,0,1,1,3,3,4,4,2,3,1,2,2,2,4,4,4]) * 1.0 #sample dataset\n",
    "\n",
    "# ordered\n",
    "TEST_LABELS = np.array([0.0, 2.0, 1.0, 0.0, 4.0, 2.0, 0.0, 4.0, 0.0, 0.0, 2.0, 2.0, 4.0, 4.0, 3.0, 3.0, 1.0, 4.0, 2.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0]) #sample dataset\n",
    "\n",
    "for i in range(1,11):\n",
    "    data = deploy_pipeline([\"ttest\"+str(i)+\".txt\"])\n",
    "    if(len(data)>25):\n",
    "        continue\n",
    "    print(model.evaluate(data,TEST_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "बत्तिको अवस्था बदल\n",
      "पङ्खाको स्तिथी बदल\n",
      "आजको मौसम बताउ\n",
      "आजको मौसम बताउ\n",
      "आजको मौसम बताउ\n",
      "आजको मौसम बताउ\n",
      "पङ्खाको स्तिथी बदल\n",
      "पङ्खाको स्तिथी बदल\n",
      "अबको समय सुनाउ\n",
      "आजको मौसम बताउ\n"
     ]
    }
   ],
   "source": [
    "data = deploy_pipeline([os.path.join(TEST_DIR,\"A.txt\")])\n",
    "predictions = np.array(ORDERED_SENTENCES)[list(map(np.argmax,model.predict_proba(data)))]\n",
    "print(*predictions, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आजको मौसम बताउ\n",
      "एउटा सङ्गित बजाउ\n",
      "आजको मौसम बताउ\n",
      "आजको मौसम बताउ\n",
      "आजको मौसम बताउ\n",
      "बत्तिको अवस्था बदल\n",
      "आजको मौसम बताउ\n",
      "आजको मौसम बताउ\n",
      "आजको मौसम बताउ\n",
      "एउटा सङ्गित बजाउ\n"
     ]
    }
   ],
   "source": [
    "data = deploy_pipeline([os.path.join(TEST_DIR,\"B.txt\")])\n",
    "predictions = np.array(ORDERED_SENTENCES)[list(map(np.argmax,model.predict_proba(data)))]\n",
    "print(*predictions, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
