{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rimesh/Files/softwareFiles/python/ml/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "# import librosa\n",
    "# import librosa.display as display\n",
    "import biosppy\n",
    "import scipy.signal as sig\n",
    "from scipy.stats import zscore\n",
    "from scipy.io import wavfile\n",
    "from scipy.fft import fft,fftfreq\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "mlp.rc(\"xtick\",labelsize=10)\n",
    "mlp.rc(\"ytick\",labelsize=10)\n",
    "mlp.rc(\"axes\",labelsize=11)\n",
    "plt.rcParams[\"figure.figsize\"] = [11,5]\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "\n",
<<<<<<< HEAD
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from tensorflow import keras\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
=======
    "# import tensorflow as tf\n",
    "# import sklearn\n",
    "# import tensorflow.keras as keras\n",
    "# from tensorflow.keras.layers import TimeDistributed\n",
    "# from tensorflow.keras.layers import LSTM\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
>>>>>>> 7651a81a704bfa6c6f2dfe1a9386288015ac4fa3
    "\n",
    "CURR_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['बत्तिको अवस्था बदल' 'आजको मौसम बताउ' 'एउटा सङ्गित बजाउ'\n",
      " 'पङ्खाको स्तिथी बदल' 'अबको समय सुनाउ' 'बत्तिको अवस्था बदल'\n",
      " 'आजको मौसम बताउ' 'एउटा सङ्गित बजाउ' 'पङ्खाको स्तिथी बदल' 'अबको समय सुनाउ'\n",
      " 'बत्तिको अवस्था बदल' 'आजको मौसम बताउ' 'एउटा सङ्गित बजाउ'\n",
      " 'पङ्खाको स्तिथी बदल' 'अबको समय सुनाउ']\n"
     ]
    }
   ],
   "source": [
    "MAIN_DIR = \".\"\n",
    "if os.path.basename(os.getcwd())!=\"Silent-Interface-for-IOT-Devices\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "DATA_DIR = os.path.join(MAIN_DIR,\"new dataset\")\n",
    "FIG_DIR = os.path.join(MAIN_DIR,\"figures\")\n",
    "PICKLE_DIR = os.path.join(MAIN_DIR,\"pickles\")\n",
    "os.makedirs(FIG_DIR,exist_ok=True)\n",
    "os.makedirs(PICKLE_DIR,exist_ok=True)\n",
    "\n",
    "# hardware definitions\n",
    "\"\"\"\n",
    "https://docs.openbci.com/docs/02Cyton/CytonDataFormat#:~:text=By%20default%2C%20our%20Arduino%20sketch,of%200.02235%20microVolts%20per%20count\n",
    "\"\"\"\n",
    "SAMPLING_RATE = 250 #Hz\n",
    "NUM_CHANNELS = 8 \n",
    "ADC_RESOLUTION = 24 #bits\n",
    "ADC_GAIN = 24.0\n",
    "REF_VOLTAGE = 4.5 #Volts\n",
    "# SCALE_FACTOR = (REF_VOLTAGE/float((pow(2,23))-1)/ADC_GAIN)*1000000.0 #micro-volts\n",
    "\n",
    "# dataset definitions\n",
    "SPEAKER = [\"RL\",\"RN\",\"SR\",\"US\"]\n",
    "SESSION = [\"session\"+str(i) for i in range(1,9)]\n",
    "MODE = [\"mentally\",\"mouthed\"]\n",
    "SENTENCES =[\"अबको समय सुनाउ\",\"एउटा सङ्गित बजाउ\",\"आजको मौसम बताउ\",\"बत्तिको अवस्था बदल\",\"पङ्खाको स्तिथी बदल\"]\n",
    "LABELS = np.array(SENTENCES)[[3, 2, 1, 4, 0, 3, 2, 1, 4, 0, 3, 2, 1, 4, 0]]\n",
    "WORDS = [\"समय\",\"सङ्गित\",\"मौसम\",\"बत्ति\",\"पङ्खा\"]\n",
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files in the dataset:  165\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob(DATA_DIR+\"/**/*.txt\",recursive=True)\n",
    "print(\"The files in the dataset: \",len(all_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(files,NORMALIZE=True,DEPLOY=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    parser function to extract utterances from .txt file and store them in a dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # PERCENTILES FOR LENGTH NORMALIZATION\n",
    "    if(NORMALIZE==True):\n",
    "        percentile_95 = 1602\n",
    "        percentile_97 = 1614\n",
    "        percentile_99 = 1648\n",
    "        percentile_100 = 1875\n",
    "    else:\n",
    "        percentile_95 = percentile_97 = percentile_99 = percentile_100 = -1\n",
    "        \n",
    "    dataset = {\"data\":[], \"speaker\":[],\"session\":[],\"labels\":[]}\n",
    "    \n",
    "    def get_data(file):\n",
    "        \n",
    "        signal = read_data(file)\n",
    "        \n",
    "        if(DEPLOY==True):\n",
    "            dataset[\"data\"].extend(signal)\n",
    "            return\n",
    "        \n",
    "        if(len(signal)!=15):\n",
    "            return\n",
    "        \n",
    "        session = file.split(\"/\")[-2]\n",
    "        speaker = file.split(\"/\")[-3]\n",
    "        \n",
    "        dataset[\"data\"].extend(signal)\n",
    "        dataset[\"speaker\"].extend([speaker]*len(signal))\n",
    "        dataset[\"session\"].extend([session]*len(signal))\n",
    "        dataset[\"labels\"].extend(LABELS)\n",
    "        \n",
    "    \n",
    "    def read_data(file):\n",
    "        f = open(file, 'r')\n",
    "        contents = map(lambda x : x.strip(), f.readlines())\n",
    "        #the file starts with '%' and some instruction before data and removing these data \n",
    "        frames_original = list(filter(lambda x : x and x[0] != '%', contents))[1:]\n",
    "        #the data row contains channels info digital trigger and accelerometer info separated by comma\n",
    "        frames_original = list(map(lambda s : list(map( lambda ss: ss.strip(), s.split(','))), frames_original))\n",
    "        # (8 channels) + digital triggers\n",
    "        # the digital trigger is in a[16], used to indicate the utterance\n",
    "        frames = list(map(lambda a: list(map(float, a[1:9])) + [float(a[16])] , frames_original))\n",
    "        frames = np.array(frames)\n",
    "        indices = []\n",
    "        signal = []\n",
    "        for index,f in enumerate(frames[:,-1]):\n",
    "            if(bool(f) ^ bool(frames[(index+1) if ((index+1)<len(frames)) else index,-1]) ):\n",
    "                indices.append(index)\n",
    "                if len(indices)>1 and len(indices)%2==0:\n",
    "                    frame_len = indices[len(indices)-1] - indices[len(indices)-2]\n",
    "                    if(frame_len<percentile_99):\n",
    "                        pad = int(np.ceil((percentile_99 - frame_len)/2))\n",
    "                    else:\n",
    "                        pad = 0\n",
    "                    left_pad = indices[len(indices)-2] - pad\n",
    "                    right_pad = indices[len(indices)-1] + pad\n",
    "                    a_frame = (frames[left_pad:right_pad,:-1])[:percentile_99]\n",
    "                    signal.append(a_frame)\n",
    "    \n",
    "        # convert to microVolts and return\n",
    "        return np.array(signal) #*SCALE_FACTOR\n",
    "        \n",
    "    for file,i in zip(files,tqdm.tqdm(range(1,len(files)+1),desc=\"PARSING DATA\")):\n",
    "        get_data(file)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Fetching raw data from pickle file ...\n",
      "Done!\n"
=======
      "PARSING DATA:  99%|█████████▉| 164/165 [01:31<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Task Accomplished\n"
>>>>>>> 7651a81a704bfa6c6f2dfe1a9386288015ac4fa3
     ]
    }
   ],
   "source": [
    "\n",
    "if \"Raw_data.pickle\" in os.listdir(PICKLE_DIR):\n",
    "    print(\"Fetching raw data from pickle file ...\")\n",
    "    all_data = pickle.load(open(os.path.join(PICKLE_DIR,\"Raw_data.pickle\"),\"rb\"))\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    all_data = parser(all_files)\n",
    "    pickle.dump(all_data,open(os.path.join(PICKLE_DIR,\"Raw_data.pickle\"),\"wb\"))\n",
    "    print(\"Write Task Accomplished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['data', 'speaker', 'session', 'labels'])\n",
<<<<<<< HEAD
      "[2310, 2310, 2310, 2310]\n"
=======
      "[2310, 2310, 2310, 2310]\n",
      "आजको मौसम बताउ\n"
>>>>>>> 7651a81a704bfa6c6f2dfe1a9386288015ac4fa3
     ]
    }
   ],
   "source": [
    "print(type(all_data))\n",
    "print(all_data.keys())\n",
    "DATA_LENGTH = len(all_data[\"labels\"])\n",
    "print([len(x[-1]) for x in all_data.items()])\n",
    "print(all_data[\"labels\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOISE ADDITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_noise(data,FACTOR=0.9):\n",
    "#     noise = np.random.randn(len(data))\n",
    "# #     return FACTOR * noise\n",
    "#     new_data = data + FACTOR * noise\n",
    "#     return new_data.astype(type(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIME SHIFTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shift(data,SHIFT_LEN=100,SHIFT_DIR =\"RIGHT\",index=0):\n",
    "    if SHIFT_DIR != \"RIGHT\":\n",
    "        SHIFT_LEN = - SHIFT_LEN\n",
    "        index = -1\n",
    "    shifted_data = np.roll(data,SHIFT_LEN)\n",
    "    if SHIFT_LEN>0:\n",
    "        shifted_data[:SHIFT_LEN] = data[index]\n",
    "    else:\n",
    "        shifted_data[SHIFT_LEN:] = data[index]\n",
    "    return shifted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(DATA_LENGTH):\n",
    "    aug_data = []\n",
    "    for j in range(8):\n",
    "        temp_data1 = time_shift(all_data[\"data\"][i][:,j],SHIFT_DIR=\"LEFT\")\n",
    "        aug_data.append(temp_data1)\n",
    "        \n",
    "    all_data[\"data\"].extend([(np.array(aug_data)).T])\n",
    "    all_data[\"labels\"].extend([all_data[\"labels\"][i]])\n",
    "    all_data[\"speaker\"].extend([all_data[\"speaker\"][i]])\n",
    "    all_data[\"session\"].extend([all_data[\"session\"][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(DATA_LENGTH):\n",
    "    aug_data = []\n",
    "    for j in range(8):\n",
    "        temp_data2 = time_shift(all_data[\"data\"][i][:,j],SHIFT_DIR=\"RIGHT\")\n",
    "        aug_data.append(temp_data2)\n",
    "        \n",
    "    all_data[\"data\"].extend([(np.array(aug_data)).T])\n",
    "    all_data[\"labels\"].extend([all_data[\"labels\"][i]])\n",
    "    all_data[\"speaker\"].extend([all_data[\"speaker\"][i]])\n",
    "    all_data[\"session\"].extend([all_data[\"session\"][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6930, 6930, 6930, 6930]\n"
     ]
    }
   ],
   "source": [
    "print([len(x[-1]) for x in all_data.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS SIGNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_pipeline(data,RICKER=False):\n",
    "\n",
    "    \"\"\"\n",
    "    CORRECT DC DRIFT --> CORRECT DC BIAS --> SMOOTHING SIGNAL --> NORMALIZE DATA --> FILTER DATA \t\n",
    "    \"\"\"\n",
    "    filter_data = []\n",
    "\n",
    "    def digital_filter(data,HPF=0.5,LPF=10,H_ORDER=4,L_ORDER=4,SR=250):\n",
    "        \"\"\"\n",
    "        HPF --> NOTCH --> LPF --> RICKER CONVOLUTION\n",
    "        \"\"\"\n",
    "\n",
    "        # highpass filter\n",
    "        f_signal = biosppy.signals.tools.filter_signal(data,ftype=\"butter\",band=\"highpass\",order=H_ORDER,sampling_rate=SR,frequency=HPF)\n",
    "        # notch filter\n",
    "        b,a = sig.iirnotch(50,30,SR)\n",
    "        f_signal = sig.lfilter(b,a,f_signal[0])\n",
    "\n",
    "        # lowpass filter\n",
    "        f_signal = biosppy.signals.tools.filter_signal(f_signal,ftype=\"butter\",band=\"lowpass\",order=L_ORDER,sampling_rate=SR,frequency=LPF)\n",
    "\n",
    "        if(RICKER==True):\n",
    "            # RICKER CONVOLUTION TO REMOVE HEARTBEAT ARTIFACTS\n",
    "            ricker_width = 35 * SR // 250\n",
    "            ricker_sigma = 4.0 * SR / 250\n",
    "            ricker = sig.ricker(ricker_width,ricker_sigma)\n",
    "            # normalize ricker\n",
    "            ricker = np.array(ricker, np.float32) / np.sum(np.abs(ricker))\n",
    "            convolution = sig.convolve(f_signal[0],ricker,mode=\"same\")\n",
    "            return (f_signal[0]-2*convolution)\n",
    "\n",
    "        return f_signal[0]\n",
    "\n",
    "    def process_signal(data):\n",
    "        f_data = []\n",
    "        for i in range(8):\n",
    "            # correction of DC drift\n",
    "            c_data = data[:,i]- data[0,i]\n",
    "\n",
    "            # correct DC bias\n",
    "            c_data = c_data - np.mean(c_data)\n",
    "\n",
    "            # normalize and filter data\n",
    "            c_data = digital_filter(c_data)\n",
    "            f_data.append(c_data)\n",
    "\n",
    "        return np.array(f_data).T\n",
    "\n",
    "    for d,i in zip(data,tqdm.tqdm(range(1,len(data)+1),desc=\"PROCESSING DATA: \")):\n",
    "        temp_data = process_signal(d)\n",
    "        filter_data.extend([temp_data])\n",
    "\n",
    "    return np.array(filter_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "PROCESSING DATA: 100%|█████████▉| 2309/2310 [02:51<00:00, 13.44it/s]\n"
=======
      "PROCESSING DATA: 100%|█████████▉| 6929/6930 [00:47<00:00, 145.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Task Accomplished\n"
>>>>>>> 7651a81a704bfa6c6f2dfe1a9386288015ac4fa3
     ]
    }
   ],
   "source": [
    "picklefile = \"Filtered_aug_data.pickle\"\n",
    "if picklefile in os.listdir(PICKLE_DIR):\n",
    "    print(\"Fetching filtered data from pickle file ...\")\n",
    "    all_data_filtered = pickle.load(open(os.path.join(PICKLE_DIR, picklefile),\"rb\"))\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    all_data_filtered = all_data.copy()\n",
    "    all_data_filtered[\"data\"] = signal_pipeline(all_data[\"data\"])\n",
    "    pickle.dump(all_data_filtered,open(os.path.join(PICKLE_DIR, picklefile),\"wb\"))\n",
    "    print(\"Write Task Accomplished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['data', 'speaker', 'session', 'labels'])\n",
<<<<<<< HEAD
      "[2310, 2310, 2310, 2310]\n"
=======
      "[6930, 6930, 6930, 6930]\n"
>>>>>>> 7651a81a704bfa6c6f2dfe1a9386288015ac4fa3
     ]
    }
   ],
   "source": [
    "print(type(all_data_filtered))\n",
    "print(all_data_filtered.keys())\n",
    "print([len(x[-1]) for x in all_data_filtered.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "(2310, 1648, 8)\n"
=======
      "(6930, 1648, 8)\n"
>>>>>>> 7651a81a704bfa6c6f2dfe1a9386288015ac4fa3
     ]
    }
   ],
   "source": [
    "print(np.array(all_data_filtered[\"data\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'speaker', 'session', 'labels'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_filtered.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 24,
>>>>>>> 7651a81a704bfa6c6f2dfe1a9386288015ac4fa3
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getSpec(sdata):\n",
    "#     win = sig.windows.hamming(50)\n",
    "#     return signal.spectrogram(x=np.array(sdata), sr=SAMPLING_RATE, window=win, nperseg=len(win))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check COLA constraint\n",
    "M = 60\n",
    "win = sig.windows.hann(M, False)\n",
    "sig.check_COLA(win, nperseg=M, noverlap=4*M/5) # 80% overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pipeline(data):\n",
    "    feature_data = []\n",
    "    def getSpect(sdata):\n",
    "        M = 60\n",
    "        win = sig.windows.hann(M, sym=False)\n",
    "        return sig.spectrogram(x=np.array(sdata), fs=SAMPLING_RATE, window=win, nperseg=len(win), noverlap=4*M/5, nfft=4*M)\n",
    "    def process_signal(data):\n",
    "        f_data = []\n",
    "        for i in range(8):\n",
    "            _, _, c_data = getSpect(data[:, i])\n",
    "            f_data.append(c_data)\n",
    "        return np.array(f_data)\n",
    "    \n",
    "    for d,i in zip(data,tqdm.tqdm(range(1,len(data)+1),desc=\"EXTRACTING DATA: \")):\n",
    "        temp_data = process_signal(d)\n",
    "        feature_data.extend([temp_data])\n",
    "    return np.array(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = feature_pipeline(all_data[\"data\"])\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EXTRACTING DATA:  29%|██▊       | 1978/6930 [00:20<00:52, 94.95it/s]"
     ]
    }
   ],
   "source": [
    "fileToImport = \"data_dict_sfeature_aug.pickle\"\n",
    "if fileToImport in os.listdir(PICKLE_DIR):\n",
    "    print(\"Fetching feature data from pickle file ...\")\n",
    "    all_data_feature = pickle.load(open(os.path.join(PICKLE_DIR, fileToImport),\"rb\"))\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    all_data_feature = all_data.copy()\n",
    "    all_data_feature[\"data\"] = feature_pipeline(all_data[\"data\"])\n",
    "    pickle.dump(all_data_feature,open(os.path.join(PICKLE_DIR, fileToImport),\"wb\"))\n",
    "    print(\"Write Task Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_feature[\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
